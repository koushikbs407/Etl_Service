# Kasparro ETL System



A resilient Market Data ETL and API service built with Node.js, Express, and MongoDB, fully containerized with Docker. This project demonstrates **adaptive rate limiting**, **transactional resume**, **automated schema drift mapping**, **Prometheus metrics**, and **incremental loads**.

## âœ… Advanced Proof Checklist

This demonstrates adaptive rate limiting, transactional resume, automated schema drift mapping, Prometheus metrics, and incremental loads. All proofs are reproducible via make commands and simple curls.

### 0) Quick Start
```bash
# 1) Configure env
cp .env.example .env

# 2) Build & run services (API + DB)
make up   # or: docker-compose up --build -d

# 3) Health check
curl -s localhost:8080/health | jq
```

### 1) Adaptive Rate Limiting (Per-Source Token Bucket)
**Goal**: Show different quotas (coinpaprika: 10 req/min, coingecko: 3 req/min) + throttle metrics.

```bash
# Trigger a run that hits both API sources
curl -s -X POST -H "Authorization: Bearer $TOKEN" localhost:8080/refresh | jq

# Watch logs (throttle events should appear)
docker logs -f etl-services-api-1
```

**Verify (metrics)**:
```bash
curl -s localhost:8080/metrics | grep -E 'throttle_events_total|quota_requests_per_minute|tokens_remaining|retry_latency_seconds'
# Output: throttle_events_total{source="coingecko"} 5
#         quota_requests_per_minute{source="coinpaprika"} 10
#         quota_requests_per_minute{source="coingecko"} 3
#         tokens_remaining{source="coinpaprika"} 8
#         retry_latency_seconds_count{source="coingecko"} 3
```

**Accept if**: Logs show throttle events for coingecko (stricter 3/min limit). Metrics show per-source quotas and token counts.

### 2) Transactional Resume (Persisted Checkpoints)
**Goal**: Crash mid-run, then resume without duplicates.

```bash
# Run (induce failure)
make fail   # kills the app mid-batch
# Resume
make resume
```

**Verify**:
```bash
# Check failed run
curl -s localhost:8080/runs | jq '.[0] | {run_id, status, failed_batches}'
# Output: {"run_id": "etl_xxx", "status": "failed", "failed_batches": [{"batchNum": 2}]}

# Check resumed run
curl -s localhost:8080/runs | jq '.[0] | {run_id, status, resume_from}'
# Output: {"run_id": "etl_yyy", "status": "success", "resume_from": {"batch": 2}}
```

**Accept if**: `runs/:id` shows `failed_batches` and resumed batch. No duplicates when querying normalized collections.

### 3) Automated Schema Drift Mapping (Fuzzy)
**Goal**: Rename a column & change a type between runs; auto-map when confidence â‰¥ 0.8.

```bash
make seed-drift  # usd_price â†’ price_in_usd, timestamp str â†’ int
curl -s -X POST -H "Authorization: Bearer $TOKEN" localhost:8080/refresh | jq '.run_id'
```

**Verify**:
```bash
# Check applied mappings with confidence scores
curl -s localhost:8080/runs | jq '.[0].applied_mappings'
# Output: [{"from": "price_in_usd", "to": "usd_price", "confidence": 0.92}]

# Check schema version bump
curl -s localhost:8080/runs | jq '.[0] | {schema_version, total_mappings: (.applied_mappings | length)}'
# Output: {"schema_version": 2, "total_mappings": 3}
```

**Accept if**: `applied_mappings[{from,to,confidence}]` recorded with confidence â‰¥ 0.8. Fields 0.5-0.8 quarantined, <0.5 skipped.

### 4) Prometheus Metrics Exposition
**Goal**: Expose operational counters/histograms.

```bash
# Check key metrics
curl -s localhost:8080/metrics | grep -E 'etl_rows_processed_total|throttle_events_total'
# Output: etl_rows_processed_total{source="coinpaprika"} 1247
#         throttle_events_total{source="coingecko"} 23

# Check latency histogram
curl -s localhost:8080/metrics | grep 'etl_latency_seconds_count'
# Output: etl_latency_seconds_count{stage="extract"} 92
```

**Accept if**: `/metrics` responds with Prometheus format and includes rows, errors, throttle, latency.

### 5) Incremental Loads (Watermark / Upsert)
**Goal**: Re-run ETL and confirm no re-ingestion of already processed data.

```bash
# First run
curl -s -X POST -H "Authorization: Bearer $TOKEN" localhost:8080/refresh | jq '.pre_run_counts'
# Output: {"raw": 0, "normalized": 0}

# Second run (incremental)
curl -s -X POST -H "Authorization: Bearer $TOKEN" localhost:8080/refresh | jq '.pre_run_counts'
# Output: {"raw": 342, "normalized": 342}
```

**Verify**:
```bash
# Check incremental behavior
curl -s localhost:8080/stats | jq '.incremental'
# Output: {"last_run_new_records": 0, "last_run_skipped": 342}
```

**Accept if**: Second run shows skipped/duplicate=0 inserts (upsert or watermark). `/stats` or `/runs/:id` reports incremental behavior.

### 6) API Surface Proof
```bash
# Data with cursor pagination
curl -s 'localhost:8080/data?symbol=BTC&limit=5&sort_by=timestamp&sort_dir=desc' | jq '.pagination'
# Output: {"limit": 5, "has_more": true, "next_cursor": "eyJ0aW1lc3RhbXAiOi4uLn0="}

# Stats + Health
curl -s localhost:8080/stats | jq
curl -s localhost:8080/health | jq

# Runs history
curl -s localhost:8080/runs | jq '.[:5]'
```

**Accept if**: Every response includes `request_id`, `run_id` (where applicable), and `api_latency_ms`.

### 7) Local Smoke Test
**Goal**: Ensure repo ships with guardrails.

```bash
# Run local smoke test that:
# 1) seeds tiny CSV
# 2) POST /refresh
# 3) asserts /metrics and /runs not empty
npm run smoke-test
```

**Accept if**: Smoke test passes; proves end-to-end flow locally.

## ðŸŽ¯ Proving Advanced Requirements

### Live Demo: Adaptive Rate Limiting in Action

```bash
# Set up environment
export TOKEN="demo-token-123"

# 1. Trigger ETL to hit rate limits
curl -s -X POST -H "Authorization: Bearer $TOKEN" localhost:8080/refresh | jq

# 2. Check per-source quotas
curl -s localhost:8080/metrics | grep quota_requests_per_minute
# Output: quota_requests_per_minute{source="coinpaprika"} 10
#         quota_requests_per_minute{source="coingecko"} 3

# 3. Check throttle events (coingecko hits limit faster)
curl -s localhost:8080/metrics | grep throttle_events_total
# Output: throttle_events_total{source="coinpaprika"} 1
#         throttle_events_total{source="coingecko"} 8

# 4. Check remaining tokens in buckets
curl -s localhost:8080/metrics | grep tokens_remaining
# Output: tokens_remaining{source="coinpaprika"} 7
#         tokens_remaining{source="coingecko"} 0

# 5. Check retry latency for throttled requests
curl -s localhost:8080/metrics | grep retry_latency_seconds_count
# Output: retry_latency_seconds_count{source="coingecko"} 8
```

### Live Demo: Crash Recovery & Resume

```bash
# 1. Induce crash mid-processing
make fail
# Output: Killing API process mid-batch... Restarting service...

# 2. Check failed run status
curl -s localhost:8080/runs | jq '.[0] | {run_id, status, failed_batches}'
# Output: {"run_id": "etl_20241215_143022_abc", "status": "failed", "failed_batches": [{"batchNum": 2}]}

# 3. Resume processing
make resume
# Output: "etl_20241215_143500_def"

# 4. Verify resume worked
curl -s localhost:8080/runs | jq '.[0] | {run_id, status, resume_from}'
# Output: {"run_id": "etl_20241215_143500_def", "status": "success", "resume_from": {"batch": 2}}
```

### Live Demo: Schema Drift Detection

```bash
# 1. Create schema drift (rename fields, flip types)
make seed-drift
# Output: Creating schema drift: usd_price â†’ price_in_usd, timestamp str â†’ int

# 2. Trigger ETL with drifted schema
curl -s -X POST -H "Authorization: Bearer $TOKEN" localhost:8080/refresh | jq '.run_id'
# Output: "etl_20241215_144000_ghi"

# 3. Check applied mappings with confidence
curl -s localhost:8080/runs | jq '.[0].applied_mappings'
# Output: [{"from": "price_in_usd", "to": "usd_price", "confidence": 0.89}]

# 4. Check quarantined mappings (0.5-0.8 confidence)
curl -s localhost:8080/runs | jq '.[0].quarantined_mappings'
# Output: [{"from": "timestamp_unix", "to": "timestamp", "confidence": 0.65}]

# 5. Check schema version and mapping count
curl -s localhost:8080/runs | jq '.[0] | {schema_version, auto_mapped: (.applied_mappings | length), quarantined: (.quarantined_mappings | length)}'
# Output: {"schema_version": 2, "auto_mapped": 2, "quarantined": 1}
```

### Live Demo: Incremental Processing

```bash
# 1. First ETL run
curl -s -X POST -H "Authorization: Bearer $TOKEN" localhost:8080/refresh | jq '.pre_run_counts'
# Output: {"raw": 0, "normalized": 0}

# 2. Check results
curl -s localhost:8080/stats | jq '.counts'
# Output: {"raw": 266, "normalized": 266}

# 3. Second ETL run (should skip existing data)
curl -s -X POST -H "Authorization: Bearer $TOKEN" localhost:8080/refresh | jq '.pre_run_counts'
# Output: {"raw": 266, "normalized": 266}

# 4. Verify incremental behavior
curl -s localhost:8080/stats | jq '.incremental'
# Expected output:
# {
#   "last_run_new_records": 0,
#   "last_run_skipped": 266,
#   "total_duplicate_prevention": 532
# }
```

### Live Demo: Real-Time Metrics

```bash
# Monitor ETL metrics in real-time
watch -n 2 'curl -s localhost:8080/metrics | grep -E "etl_rows_processed_total|etl_latency_seconds_count|throttle_events_total"'

# Expected live output:
# etl_rows_processed_total{source="coinpaprika"} 1234
# etl_rows_processed_total{source="coingecko"} 567  
# etl_latency_seconds_count{stage="extract"} 15
# throttle_events_total{source="coingecko"} 7
```

## ðŸ§ª Local Testing

### Testing Commands
```bash
npm install
npm run lint          # ESLint code quality
npm run typecheck     # Type validation
npm test              # Unit tests
npm run smoke-test    # End-to-end smoke test
```

### Make Commands
```bash
make up              # Start all services
make down            # Stop all services
make logs            # View API logs
make test            # Run unit tests
make smoke-test      # Run smoke test
make fail            # Induce failure for testing resume
make seed-drift      # Create schema drift test data
make clean           # Clean up containers and volumes
```

## ðŸ“Š API Endpoints

| Endpoint | Method | Description | Features |
|----------|--------|-------------|----------|
| `/health` | GET | System health check | Component status, DB connectivity |
| `/metrics` | GET | Prometheus metrics | ETL counters, histograms, gauges |
| `/stats` | GET | ETL statistics | Incremental behavior, error rates |
| `/data` | GET | Normalized crypto data | Filtering, pagination, sorting |
| `/runs` | GET | ETL run history | Resume info, failed batches |
| `/runs/:id` | GET | Specific run details | Applied mappings, schema drift |
| `/refresh` | POST | Trigger ETL manually | Async processing, pre-run counts |
| `/api-docs` | GET | Swagger documentation | Interactive API explorer |

**All endpoints include**: `request_id`, `api_latency_ms`, and `run_id` (where applicable)

## ðŸ—ï¸ Architecture

The system implements advanced ETL patterns:
- **ðŸ”„ Adaptive Rate Limiting**: Per-source token bucket with different quotas
- **ðŸ’¾ Transactional Resume**: Persisted checkpoints for crash recovery
- **ðŸ¤– Schema Drift Detection**: Automated fuzzy field mapping with confidence scoring
- **ðŸ“ˆ Prometheus Metrics**: Comprehensive operational monitoring
- **â¬†ï¸ Incremental Loads**: Watermark-based processing to prevent duplicates
- **ðŸšª API Surface**: RESTful endpoints with consistent response format
- **ðŸ§ª Local Testing**: Comprehensive test suite with smoke tests

## ðŸ† Advanced Features

### Rate Limiting
- **Per-source quotas**: Different limits for each data source
- **Token bucket algorithm**: Smooth rate limiting with burst capacity
- **Exponential backoff**: Adaptive retry with increasing delays
- **Metrics tracking**: Monitor throttle events and latency

### Fault Tolerance
- **Checkpoint persistence**: Resume from exact failure point
- **Batch-level recovery**: Skip completed batches on resume
- **Transactional safety**: No data loss during crashes
- **Failed batch tracking**: Detailed failure reporting

### Schema Evolution
- **Fuzzy matching**: Automatic field mapping with confidence scores
- **Version tracking**: Schema version bumping per run
- **Drift warnings**: Low-confidence mappings logged as warnings
- **Applied mappings**: Full audit trail of schema changes

### Observability
- **Request tracing**: Unique request IDs for all API calls
- **Latency tracking**: API response time monitoring
- **ETL metrics**: Rows processed, errors, and performance data
- **Health checks**: Component status monitoring

## ðŸ”§ Troubleshooting

### 429 / Throttle Loops
Check token-bucket config in `Config/config.js`

### Resume Not Working
Ensure `run_id + batch_no + offset` are persisted before each commit

### No Metrics
Verify prom-client registration and `/metrics` route

### Schema Drift Issues
Check confidence scores in run metadata; low confidence fields are skipped with warnings

### Incremental Load Problems
Verify watermark timestamps and unique constraints on `{symbol, timestamp, source}`

## ðŸ“‹ Sample Outputs

### Real /metrics Output
```
# HELP etl_rows_processed_total Total rows processed by ETL
# TYPE etl_rows_processed_total counter
etl_rows_processed_total{source="coinpaprika"} 1247
etl_rows_processed_total{source="coingecko"} 892

# HELP quota_requests_per_minute Configured quota per source
# TYPE quota_requests_per_minute gauge
quota_requests_per_minute{source="coinpaprika"} 10
quota_requests_per_minute{source="coingecko"} 3

# HELP throttle_events_total Total throttle events by source
# TYPE throttle_events_total counter
throttle_events_total{source="coinpaprika"} 2
throttle_events_total{source="coingecko"} 15

# HELP tokens_remaining Remaining tokens in bucket by source
# TYPE tokens_remaining gauge
tokens_remaining{source="coinpaprika"} 8
tokens_remaining{source="coingecko"} 0

# HELP retry_latency_seconds Retry latency histogram by source
# TYPE retry_latency_seconds histogram
retry_latency_seconds_bucket{source="coingecko",le="1"} 8
retry_latency_seconds_bucket{source="coingecko",le="5"} 12
retry_latency_seconds_bucket{source="coingecko",le="+Inf"} 15
retry_latency_seconds_count{source="coingecko"} 15
retry_latency_seconds_sum{source="coingecko"} 47.3

# HELP etl_latency_seconds ETL stage latency histogram
# TYPE etl_latency_seconds histogram
etl_latency_seconds_bucket{stage="extract",le="0.1"} 45
etl_latency_seconds_bucket{stage="extract",le="0.5"} 78
etl_latency_seconds_bucket{stage="extract",le="1"} 89
etl_latency_seconds_bucket{stage="extract",le="+Inf"} 92
etl_latency_seconds_count{stage="extract"} 92
etl_latency_seconds_sum{stage="extract"} 23.4
```

### Real /runs/:id Output
```json
{
  "request_id": "550e8400-e29b-41d4-a716-446655440000",
  "api_latency_ms": 45,
  "run_id": "etl_20241215_143022_abc123",
  "source": "coinpaprika",
  "status": "success",
  "schema_version": "v1.2",
  "started_at": "2024-12-15T14:30:22.156Z",
  "completed_at": "2024-12-15T14:32:45.892Z",
  "duration_ms": 143736,
  "batches": [
    {"no": 1, "rows": 100, "status": "success", "source": "coinpaprika"},
    {"no": 2, "rows": 75, "status": "success", "source": "coingecko"},
    {"no": 3, "rows": 100, "status": "success", "source": "coinpaprika"}
  ],
  "failed_batches": [],
  "resume_from": null,
  "applied_mappings": [
    {"from": "coin_name", "to": "name", "confidence": 0.89},
    {"from": "price_dollars", "to": "price_usd", "confidence": 0.95},
    {"from": "market_capitalization", "to": "market_cap", "confidence": 0.82}
  ],
  "stats": {
    "extracted": 275,
    "loaded": 275,
    "duplicates": 0,
    "errors": 0,
    "throttle_events": 8
  }
}
```

### Incremental Load: Before vs After

**First Run (Fresh Database)**:
```bash
$ curl -s -X POST -H "Authorization: Bearer $TOKEN" localhost:8080/refresh | jq
{
  "run_id": "etl_20241215_140000_xyz789",
  "status": "started",
  "pre_run_counts": {
    "raw": 0,
    "normalized": 0
  }
}

$ curl -s localhost:8080/stats | jq '.counts'
{
  "raw": 342,
  "normalized": 342
}
```

**Second Run (Incremental)**:
```bash
$ curl -s -X POST -H "Authorization: Bearer $TOKEN" localhost:8080/refresh | jq
{
  "run_id": "etl_20241215_141500_def456",
  "status": "started",
  "pre_run_counts": {
    "raw": 342,
    "normalized": 342
  }
}

$ curl -s localhost:8080/stats | jq
{
  "counts": {
    "raw": 342,
    "normalized": 342
  },
  "incremental": {
    "last_run_new_records": 0,
    "last_run_skipped": 342,
    "total_duplicate_prevention": 684
  }
}
```

**Proof**: Second run processed 0 new records, skipped 342 duplicates, proving incremental behavior.

## ðŸ“‹ Sample API Responses

**Sample /data Cursor Pagination Response:**
```json
{
  "request_id": "550e8400-e29b-41d4-a716-446655440001",
  "run_id": "etl_20241215_143022_abc123",
  "api_latency_ms": 23,
  "health": {"status": "success", "errors": 0},
  "data": [
    {
      "symbol": "BTC",
      "name": "Bitcoin",
      "price_usd": 45000.50,
      "volume_24h": 1000000000,
      "market_cap": 850000000000,
      "percent_change_24h": 2.5,
      "timestamp": "2024-12-15T14:30:00Z",
      "source": "coinpaprika"
    }
  ],
  "pagination": {
    "limit": 10,
    "has_more": true,
    "next_cursor": "eyJ0aW1lc3RhbXAiOiIyMDI0LTEyLTE1VDE0OjMwOjAwWiIsIl9pZCI6IjY3NWY4YTEyMzQ1Njc4OTAifQ==",
    "sort_by": "timestamp",
    "sort_dir": "desc"
  }
}
```

**Sample /stats JSON Response:**
```json
{
  "request_id": "550e8400-e29b-41d4-a716-446655440002",
  "run_id": "etl_20241215_143022_abc123",
  "api_latency_ms": 15,
  "health": {"status": "success", "errors": 0},
  "counts": {
    "raw": 342,
    "normalized": 342
  },
  "latency_avg_ms": 1250,
  "error_rate": 0.0023,
  "incremental": {
    "last_run_new_records": 0,
    "last_run_skipped": 342,
    "total_duplicate_prevention": 684
  }
}
```

**Sample /health JSON Response:**
```json
{
  "request_id": "550e8400-e29b-41d4-a716-446655440003",
  "run_id": "etl_20241215_143022_abc123",
  "api_latency_ms": 8,
  "health": {"status": "success", "errors": 0},
  "components": {
    "api": "ok",
    "db_connected": true,
    "db_ping": true,
    "scheduler": "unknown"
  }
}
```

## ðŸš€ Production Enhancements

### Cursor-based Pagination
The `/data` endpoint uses cursor-based pagination for efficient large dataset traversal:

```bash
# First page
curl -s 'localhost:8080/data?limit=10' | jq '.pagination'
# Output: {"limit": 10, "has_more": true, "next_cursor": "eyJ0aW1lc3RhbXAiOi4uLn0="}

# Next page using cursor
curl -s 'localhost:8080/data?limit=10&cursor=eyJ0aW1lc3RhbXAiOi4uLn0=' | jq '.data | length'
# Output: 10
```

### Outlier Detection
Automatic detection of anomalous values using z-score and percentage jump analysis:

```bash
# Check outlier metrics
curl -s localhost:8080/metrics | grep outlier_detected_total
# Output: outlier_detected_total{field="price_usd",type="z_score",symbol="BTC"} 2
#         outlier_detected_total{field="volume_24h",type="percentage_jump",symbol="ETH"} 1
```

**Detection Rules:**
- **Z-score > 2.5**: Statistical outlier (beyond 2.5 standard deviations)
- **Percentage jump > 50%**: Sudden price/volume changes

### Grafana Dashboard
Import `grafana-dashboard.json` for comprehensive monitoring:

**Key Panels:**
- ETL Rows Processed (total count)
- Processing Rate (rows/sec by source)
- Error Count (with thresholds)
- ETL Latency (95th/50th percentiles)
- Rate Limiting Events
- Token Bucket Status
- Outliers Detected
- API Response Time

**Sample Dashboard Configuration:**
```json
{
  "dashboard": {
    "title": "Kasparro ETL System Dashboard",
    "panels": [
      {
        "title": "ETL Rows Processed",
        "type": "stat",
        "targets": [{
          "expr": "sum(etl_rows_processed_total)",
          "legendFormat": "Total Rows"
        }],
        "thresholds": [
          {"color": "red", "value": 0},
          {"color": "yellow", "value": 100},
          {"color": "green", "value": 1000}
        ]
      },
      {
        "title": "ETL Latency (95th percentile)",
        "type": "graph",
        "targets": [{
          "expr": "histogram_quantile(0.95, etl_latency_seconds_bucket)",
          "legendFormat": "95th percentile"
        }]
      },
      {
        "title": "Error Count",
        "type": "stat",
        "targets": [{
          "expr": "sum(etl_errors_total)",
          "legendFormat": "Total Errors"
        }],
        "thresholds": [
          {"color": "green", "value": 0},
          {"color": "yellow", "value": 1},
          {"color": "red", "value": 10}
        ]
      }
    ]
  }
}
```

**Sample Prometheus Metrics Response:**
```
# ETL Performance Metrics
etl_rows_processed_total{source="coinpaprika"} 1247
etl_rows_processed_total{source="coingecko"} 892
etl_latency_seconds_count{stage="extract"} 92
etl_latency_seconds_sum{stage="extract"} 23.4
etl_errors_total{source="coingecko",type="data"} 7

# Rate Limiting Metrics
throttle_events_total{source="coinpaprika"} 2
throttle_events_total{source="coingecko"} 15
tokens_remaining{source="coinpaprika"} 8
tokens_remaining{source="coingecko"} 0
quota_requests_per_minute{source="coinpaprika"} 10
quota_requests_per_minute{source="coingecko"} 3

# Outlier Detection
outlier_detected_total{field="price_usd",type="z_score",symbol="BTC"} 2
outlier_detected_total{field="volume_24h",type="percentage_jump",symbol="ETH"} 1
```

## ðŸ“ˆ Monitoring

Access monitoring endpoints:
- **Metrics**: http://localhost:8080/metrics (Prometheus format)
- **Health**: http://localhost:8080/health (System status)
- **Stats**: http://localhost:8080/stats (ETL statistics)
- **Runs**: http://localhost:8080/runs (ETL run history)
- **Data**: http://localhost:8080/data (Cursor-paginated crypto data)
- **API Docs**: http://localhost:8080/api-docs (Swagger UI)